---
title: "HarvardX; Data Science Capstone"
subtitle: 
  - "Choose Your Own Project"
  - "Mushroom Classification"
author: "Tali Behar"
date: "December 2020"
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center", out.width = '65%',
                      message=FALSE, warning=FALSE)
```
# Executive Summary

## Overview

we like to walk increased the walking during covid time, we encounter mushrooms when walking in the woods in our area, last fall our yard was full of mushrooms that popped up during the night. 
different typs in different areas, according to the type of soil, sun exposure, grass. 
we wonder about their type. 
<to do> take some quotes from Wiki 

## Background
 < from kaggle > 
this dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like "leaflets three, let it be'' for Poisonous Oak and Ivy.

The data is categorical. 
Each column contain it's own factors, represented by unique letters (see extension on 
section... < TODO>)

## Goal
 < from kaggle > 

What types of machine learning models perform best on this dataset?
Which features are most indicative of a poisonous mushroom?

## Variable Selection

1.	**Y = class**
    The outcome will be the class of the mushroom: e = edible, p = poisonous 

2.	**The features**

  For classifing mushrooms, wheater it's outside or through pictures it's important to be femiliar with all the details about them. I choose to take all of the 22 mushrooms characteristics to be used for the prediction; odor, population,	habitat, bruises, cap, gill, stalk, veil, ring and spore-print-color.
    
## Method 
  
The ML algorithms that was chosen to model the mushroom classification data set comes from the family of **Classification Algorithms** and are: Random forest, Classification trees, and K-nearest neighbors    

 * Divide the mushrooms dataset randomly to train set and test set 
 
 * Visualize the distribution of the characteristics between e/p class and find thoghtful insights 
 
 * Visualize the relationships and correlation in the data - once between the characteristics and the class, and once among the characteristics themselves. 
 
 * Train and fit the above algorithms using cross-validation and tuning methods
 
 * Constract bulian data set for handling with Knn algorithm  

## Model Estimation

Evaluate model performance based on its **overall accuracy** 

Overall accuracy is the simplest way to evaluate the algorithm when the outcomes are categorical. This method is simply reporting the proportion of cases that were correctly predicted in the test set.  

## Model Results 

## Conclusion
 
 
# Data Preperation

## Installing Required Packeges

```{r required packeges, eval=FALSE, include=FALSE}
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(plot.matrix)) install.packages("plot.matrix", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
```

```{r install required packeges, echo=TRUE}
library(readr)
library(tidyverse)
library(caret)
library(gridExtra)
library(knitr)
library(plot.matrix)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(randomForest)
```

## Loading Mushroom Classification Dataset
```{r}
# Importing the data - read csv file 
# TODO: change to local file
mushrooms <- read.csv("D:/OneDrive/Documents/Tali/Tali data science studies/MushroomClassification/mushrooms.csv")
```

## Get A Glimpse Of Mushroom Classification Dataset
```{r dim table, echo=FALSE}
tibble("class"=class(mushrooms), "nrow"=nrow(mushrooms), "ncol"=ncol(mushrooms)) %>%
  knitr::kable()
```

```{r glimpse of the data, echo=FALSE}
glimpse(mushrooms)
```
### The mushrooms datadet contains 8k rows and 23 columns. 

**columns** 

The first column represent the class of the mushroom "e" for edible and "P" for poisonous.

Columns 2:23 represents the mushrooms chcharacteristics as the following: 

Odor, Population,	Habitat, Bruises, Cap, Gill, Stalk, Veil, Ring and Spore-print-color. 

We can distinguish between visual and general characteristics: 

  * Visual characteristics can be measured from a picture, such as the color and shaps of the mushroom's cap, bruises, gill, stalk, veil and ring. 

  * General characteristics can be evaluated in-person, such as the mushroom's odor, the habitat, population and Spore-print-color. 

**Rows** 

Each row represent one observation on one mushroom and comtain its class and unique characteristics.


Moving forward, we'll learn about the mushrooms chcharacteristics and their influence on the mashroom's claclassification.

### The dataset is categoriacl
  
The next table specify the class of the outcome and featurs (factor), and the levels of each of the factors. 
  
'veil.type' has only 1 level which means not contributing info as a feature. We'll remove it from the data and won't use it as a feature. 

   <TODO> look for NA's , write about "?"
```{r class and levels summarize , echo=FALSE}
cbind.data.frame("mushrooms characteristics"= names(mushrooms),
                 "class"= sapply(mushrooms,(function(x){class(x)})), 
                 "levels"= sapply(mushrooms,function(x){nlevels(x)})) %>% 
  mutate(char = rownames(.)) %>% 
  select(-char)%>%
  knitr::kable()
```

```{r remove veil type col, include=FALSE}
mushrooms$veil.type<-NULL
```
### The prevalence of the observation

The prevalence of the mashrooms class in the dataset is almost about the same .
  
52% of the observations belong to edible type and 48% are poisonous, which means we should not expect biased prediction. 

Predicting poisonous mushroom as edible can be a terrible mistake, so in addition to the accuracy of the model, we'll also take a look of the other parts of the 'confusionMatrix', that contains the sensitivity, specificity, prevalence and balanced accuracy.    
```{r mushroom type, echo=FALSE}
mushrooms %>% 
  group_by(class) %>% 
  summarize (count=n())%>%
  mutate(prop=100*count/sum(count)) %>%
  setNames(c("Mushroom class","Count", "Proportion (%)")) %>%
  knitr::kable()
```

Here is a quick summary of the variables and their count. 

since the full table is very wide, we'll present only the first 5 columns
```{r dataset summary, echo=FALSE}
mushrooms[,1:5] %>%
  summary()
```

## Create training and test sets
```{r create train and test}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# Test set will be 20% of mushrooms data
test_index <- createDataPartition(y = mushrooms$class, 
                                  times = 1, p = 0.2, list = FALSE)

train_set <- mushrooms %>% slice(-test_index)
test_set <- mushrooms %>% slice(test_index)
# Remove test_index
rm(test_index)

# Define the outcome (class of the mushrooms) = y  
y <- train_set$class

# Restore the true y from test set
y_test <- test_set$class

# Remove the outcome from training and test set
test_set <- test_set %>% select(-class)
train_set <- train_set %>% select(-class)
```

# Mushroom classification Dataset Overview 

## Visual Characteristics

### cap and bruises characteristics

```{r figure 1 cap characteristics, echo=FALSE, out.width = '75%'}
cap_char <- 
  lapply(names(train_set[,1:4]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 4 cap figures in 2 columns and 2 rows 
grid.arrange(cap_char[[1]],cap_char[[2]],cap_char[[3]],cap_char[[4]],
             ncol=2, nrow=2)
```
  * Cap
  
    - Differently shaped and colored upper part of the mushroom that protects the gills. In other words, the cap is the top of the mushroom. 

    - Cap shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s

    - Cap surface: fibrous=f, grooves=g, scaly=y, smooth=s

    - Cap color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y


  * Bruises

    - Mushroom bruising involves nicking the top and bottom of the mushroom cap and observing any colour changes

    - Bruises: bruises=t, no=f
    
    
  * We can see that there is no significant difference in the cap characteristics or bruises that indicate class. Yet, sunken shape, purple color and green color will always indicate edible mushroom.
  
### gill characteristics
```{r figure 2 gill characteristics, echo=FALSE, out.width = '75%'}
gill_char <- 
  lapply(names(train_set[,6:9]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 4 gill figures in 2 columns and 2 rows 
grid.arrange(gill_char[[1]],gill_char[[2]],gill_char[[3]],gill_char[[4]],
             ncol=2, nrow=2)
```
  * Fertile spore-producing part of the mushroom, located under the cap. 

    - Gill attachment: attached=a, descending=d, free=f, notched=n

    - Gill spacing: close=c, crowded=w, distant=d

    - Gill size: broad=b, narrow=n

    - Gill color: black=k, brown=n, buff=b, chocolate=h, gray=g,
    green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y
 
 
  * We can see differences in the gill characteristics that may indicate the mushrooms class. Buff and Green gill colors will always indicate poisonous.   

### stalk characteristics
```{r figure 3 stalk characteristics, echo=FALSE, out.width = '75%'}
stalk_char <- 
  lapply(names(train_set[,10:15]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 6 stalk figures in 2 columns and 3 rows 
grid.arrange(stalk_char[[1]],stalk_char[[2]],stalk_char[[3]],
             stalk_char[[4]],stalk_char[[5]],stalk_char[[6]],
             ncol=2, nrow=3)
```
  * Stem, Axis supporting the mushroomâ€™s cap. The role of the mushroom stalk is to raise the cap above the grass, twigs or stones that are close to the ground
  
    - Stalk shape: enlarging=e, tapering=t

    - Stalk root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?

    - Stalk surface above ring: fibrous=f, scaly=y, silky=k, smooth=s

    - Stalk surface below ring: fibrous=f, scaly=y, silky=k, smooth=s

    - Stalk color above ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y

    - Stalk color below ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y


  * Buff, cinnamon and yellow stalk color above ring will always indicate poisonous mushroom. All other stalk characteristics distributing in different frequancy between the mushrooms class.  

### veil and Ring characteristics
```{r figure 4 veil ring characteristics, echo=FALSE, out.width = '75%'}
veil_ring_char <- 
  lapply(names(train_set[,16:18]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
  })
# plot all 3 veil figures in 2 columns and 2 rows 
grid.arrange(veil_ring_char[[1]],veil_ring_char[[2]],
             veil_ring_char[[3]], ncol=2, nrow=2)
```
  * Veil
  
    - The thin membrane that covers the cap and stalk of an immature mushroom

    - Veil color: brown=n, orange=o, white=w, yellow=y
    
    
  * Ring
  
    - Membrane located under the cap and circling the stem; remnant of a membrane that covered the gills of the immature mushroom and ruptured as the cap grew.

    - Ring number: none=n, one=o, two=t

    - Ring type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z


  * Brown and orange veil color will always indicate edible mushroom
  
  * Mushrooms with large ring or no rings will always indicate poison.   
  
## General Characteristics
```{r figure 5 other characteristics, echo=FALSE, out.width = '75%'}
other <- 
  lapply(names(train_set[,c(5,19:21)]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
  })
# plot 4 assorted figures in 2 columns and 2 rows 
grid.arrange(other[[1]],other[[2]],other[[3]],other[[4]], ncol=2, nrow=2)
```
  * Odor - The smell of the mushroom

    - almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s
    
    - There are significant differens in the odor distribution. Poisonus and edible mushrooms smelles very diffrently and we can consider the odor as a dominant feature. 
    
    
  * Spore print color 
  
    - The spore print is the powdery deposit obtained by allowing spores of a fungal fruit body to fall onto a surface underneath
  
    - black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y
    
    - Green print color will always indicate poison. 
    
    
  * Population - the way they spread in their habitat 
  
    - abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y
    
    - Poisonus mushrooms can never be found abundant or in numerous numbers
  
  
  * Habitat
  
    - grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d
    
    - Poisonus mushrooms can grow anywhere but can never be found on waste


## Relationships and correlation in the data 

All the above characteristics are building one mushroom. since that, It can be assumed that all the feathures are correlated with the class of the mushroom and among themselves.

In the above plots we noticed high influence of the odor on the mushroom class. 
Yet, in order to select the best other featurs that have a strong predictive power, we need to see if they are correlated or not.  

For building efficient predictive models, we would ideally only include variables that uniquely explain some amount of variance in the outcome.

### Chi-Square Test

The **Chi-Square Test** (${\chi}^2$ test) of Independence determines whether there is an association between **categorical variables**.

It compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another. 

It is a nonparametric test.

A chi square test will produce a p-value which will tell us if the test results are significant or not.

Under the null hypothesis $H_{0}$ - the categorical variables are independent

If $p.value <= {\alpha}$: significant result, reject $H_{0}$, dependent.

If $p.value > {\alpha}$: not significant result, fail to reject $H_{0}$, independent.

We'll set $p.value = 0.5$ 


### Relationships between the characteristics by ${\chi}^2$ test 
```{r car p.value matrix, include=FALSE}
index <- expand.grid(1:21,1:21) 
chi2_test_for_char <- 
  sapply(1:nrow(index), function(i){
  n <- index[i,1]
  m <- index[i,2]
  chisq.test(train_set[,n],train_set[,m],
             simulate.p.value = TRUE, B = 1000)$p.value
}) %>% 
  matrix(nrow=21, byrow = FALSE)
```
The row and the columns represent the 21 features. The diagonal represent the correlation of the feature with himself (always correlated).  

We can see that all the features correlated with each other except 'veil color' and 'ring number' that are indepandent from each other  
```{r figure 6 car p.value matrix, echo=FALSE, out.width = '80%'}
plot(chi2_test_for_char, 
     col=c("white", "lightblue"),
     breaks=c(0, 0.05, 1),
     key=list(side=3, cex.axis=0.6))
```

### Relationships between the characteristics to class by ${\chi}^2$ test 
```{r car calss p.value matrix, include=FALSE} 
chi2_test_for_class <- 
  sapply(1:21, function(i){
  chisq.test(train_set[,i],y,
  simulate.p.value = TRUE, B = 1000)$p.value # B= number of monte-carlo simulations 
}) %>% 
  matrix(ncol=21, byrow = FALSE)
```
All the characteristics are corraleted with the mushroom class 
```{r figure 7 car class p.value matrix, echo=FALSE, out.width = '60%'}
plot(chi2_test_for_class, 
     col=c("white", "lightblue"), 
     breaks=c(0, 0.05, 1),
     key=list(side=3, cex.axis=0.6))
```

# Methods and analysis

A classification algorithm is a function that weighs the input features so that the output separates one class into positive values and the other into negative values. Classifier training is performed to identify the weights that provide the most accurate and best separation of the two classes of data. <TODO> write in other words

Among the common classification algorithms are clssification/desicion trees, Random forest, and Knn. The later works slight differntly than the two first.  

Since the correlation matrix showed that the featurs are dependent, we will start with all the 21 featurs. 

## Model 1 - Classification (decision) trees

  * Classification / decision trees, are used in prediction problems where the outcome is
categorical. We form predictions by calculating which class is the most common
among the training set observations within the partition.

  * The default splitting method for classification called "Gini Index" and is the following: 

$$ Gini(j) = {\displaystyle\sum_{k=1}^K\hat{p}_{j,k}(1-\hat{p}_{j,k})} $$
When $\hat{p}_{j,k}$= the proportion of observation in partition ${j}$ that are in class ${k}$

The "Gini Index" seek to partition observation into subset that have the same class. For this case, a partition have only one class, so if it's the first one, then $\hat{p}_{1,k}=1 , \hat{p}_{2,k}=0 , \hat{p}_{3,k}=0 .... \hat{p}_{j,k}=0$  

```{r eval=FALSE, include=FALSE}
# (https://www.edureka.co/blog/implementation-of-decision-tree/) 
# http://www.milbo.org/rpart-plot/prp.pdf # rpart plots examples
# https://csantill.github.io/RTuningModelParameters/
# add explenation on cp from 31.10.3 (p.589)
```
  * Train ct model and find best parameters
```{r train ct model}
train_control <- trainControl(method="cv", number = 10, p = 0.8) # use 10-folds cross-validation 
tune_grid <- data.frame(cp = seq(0.0, 0.2, len=30)) # find the best value for cp

# Train using "caret"
train_ct <- train(train_set, y, 
                  method = "rpart", 
                  tuneLength = 6,
                  trControl = train_control,
                  tuneGrid = tune_grid)
# Find best cp value
train_ct$bestTune
```
The complexity parameter (cp) is the minimum improvement in the model needed at each node. 

When cp=0 than the tree is fully grown and no need to prune it.  
```{r figure 8 Plot the cp selection by its accuracy, echo=FALSE}
plot(train_ct)
```
  * Fit the model with best parameter 
```{r fit ct model}  
fit_ct <- 
  rpart(y ~ ., 
        data = train_set, 
        method = "class", 
        control = rpart.control(cp = train_ct$bestTune$cp , minsplit = 20))
```

```{r figure 9 Plot size of the tree (nodes) by cp, echo=FALSE}
plotcp(fit_ct)
```
The above graph shows that the cross validation relative error (X-val), reaching the minimal value for a tree of size 6, which correspond to a complexity parameter of 0. 

The final tree is the following. The top node contain the entire sample, each of the remaining nodes contains a subset of the sample in the node directly above it. 

As we predicted before, Odor is a dominant feature and is the first to be splited. 
```{r figure 10 Plot the tree, echo=FALSE}
rpart.plot(x = fit_ct, type =5, extra = 4, 
           box.palette = c("lightblue","orangered"), 
           fallen.leaves=TRUE, tweak = 2)
```
  * Predict the outcome
```{r predict ct }
y_hat_ct <- predict(fit_ct, test_set, type = "class")
```
  * Report accuracy 
```{r report ct accuracy}
cm_ct <- confusionMatrix(y_hat_ct, y_test)
cm_ct
```

```{r model_1_accuracy, include=FALSE}
model_1_accuracy <- cm_ct$overall["Accuracy"]
```
  * variable importhance list - ct
```{r variable importhance list ct, echo=FALSE}
varImp(fit_ct) %>%
  mutate(char = rownames(.)) %>% 
  arrange(desc(varImp(fit_ct)$"Overall"))
```

```{r figure 11 Graph variable importhance ct, echo=FALSE}
varImp(fit_ct)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill="lightblue")+
  coord_flip()+
  labs(title = "Variable importance for classification trees model", 
       x = "characteristics", y = "variable importance")
```

## Model 1.1 - Classification (decision) trees - tuned by rf var importance

### create new dataset, eliminate the less important char by rf
```{r new_mashrooms dataset}
new_mashrooms <- 
  mushrooms %>% 
  select(-gill.attachment,-veil.color,
         -cap.shape,-cap.surface,-stalk.shape,
         -ring.number,-stalk.color.below.ring,
         -stalk.color.above.ring,-gill.spacing)
```

### split the new data into training and test sets 
```{r split the new data }
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# Test_new will be 20% of new_mashrooms data
new_test_index <- createDataPartition(y = new_mashrooms$class, times = 1, p = 0.2, list = FALSE)

train_new <- new_mashrooms %>% slice(-new_test_index)
test_new <- new_mashrooms %>% slice(new_test_index)
# Remove test_index
rm(new_test_index)

# Define the outcome (class of the mushrooms) = y_new  
y_new <- train_new$class

# Restore the true y from test_new
y_test_new <- test_new$class

# Remove the outcome from training and test_new
test_new <- test_new %>% select(-class)
train_new <- train_new %>% select(-class)
```

### Train tuned ct model and find best parameters

  * Train using "caret"
```{r Train tuned ct using caret }
train_control <- trainControl(method="cv", number = 10, p = 0.8)
tune_grid <- data.frame(cp = seq(0.0, 0.2, len=30))
train_ct_tuned <- train(train_new, y_new, 
                  method = "rpart", 
                  tuneLength = 6,
                  trControl = train_control,
                  tuneGrid = tune_grid)
# Find best cp value
train_ct_tuned$bestTune
```
  * Fit the model with best parameter 
```{r Fit tuned ct using rpart}
fit_ct_tuned <- 
  rpart(y_new ~ ., 
        data = train_new, 
        method = "class", 
        control = rpart.control(cp = train_ct_tuned$bestTune$cp , minsplit = 20))
```

```{r figure 12 Plot size of the tree (nodes) by cp, echo=FALSE}
plotcp(fit_ct_tuned)
```
  * The tuned tree 
```{r figure 13 plot tuned tree, echo=FALSE}
rpart.plot(x = fit_ct_tuned, type =5, extra = 4, 
           box.palette = c("lightblue","orangered"), 
           fallen.leaves=TRUE, tweak = 2)
```
  * Predict the outcome
```{r predict tuned ct}
y_hat_ct_tuned <- predict(fit_ct_tuned, test_new, type = "class")
```
  * Report accuracy 
```{r report tuned ct accuracy }
cm_ct_tuned <- confusionMatrix(y_hat_ct_tuned, y_test_new)
cm_ct_tuned
```

```{r tuned ct accuracy, include=FALSE}
model_1.1_accuracy <- cm_ct_tuned$overall["Accuracy"]
``` 
  * variable importhance list - ct tuned
```{r variable importhance list ct tuned, echo=FALSE}
varImp(fit_ct_tuned) %>%
  mutate(char = rownames(.)) %>% 
  arrange(desc(varImp(fit_ct_tuned)$"Overall"))
```

```{r figure 14 variable importhance tuned ct tuned, echo=FALSE}
varImp(fit_ct_tuned)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill="lightblue")+
  coord_flip()+
  labs(title = "Variable importance for classification trees - tuned model", 
       x = "characteristics", y = "variable importance")
```
no change
```{r remove the next objects, include=FALSE}
rm(test_new, train_new, y_new, y_test_new)
```
<to do> add the model limitation from the book 


## Model 2 - Random Forest

  * Train rf model and find best parameters
```{r train rf model}
set.seed(3, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(3)`

train_control <- trainControl(method="cv", number = 10, p = 0.8) # use 10-folds cross-validation 
tune_grid <- data.frame(mtry =  c(1:5)) # find the best value for tuning mtry

# Train using "caret"
set.seed(13, sample.kind="Rounding") # if using R 3.5 or earlier, use 'set.seed(13)'

train_rf <- train(train_set, y,
                  method = "rf",
                  ntree = 200, 
                  trControl = train_control,
                  tuneGrid = tune_grid)
# Find best mtry parameter
train_rf$bestTune
```

```{r figure 15 Plot the mtry selection by its accuracy, echo=FALSE}
plot(train_rf)
```
  * Fit the model with best parameter 
```{r fit rf model}
fit_rf <- randomForest(train_set, y,
                       ntree = 200, 
                       nodesize = train_rf$bestTune$mtry)
``` 

```{r figure 16 Plot ntree, echo=FALSE}
plot(fit_rf)
```
  * Predict the outcome 
```{r predict rf}  
y_hat_rf <- predict(fit_rf, test_set, type = "class")
```
  * Report accuracy 
```{r report rf accuracy}
cm_rf <- confusionMatrix(y_hat_rf, y_test)
cm_rf
```

```{r model_2_accuracy, include=FALSE}
model_2_accuracy <- cm_rf$overall["Accuracy"]
```

  * variable importhance list - rf
```{r var imp rf, echo=FALSE}
varImp(fit_rf) %>%
  mutate(char = rownames(.)) %>% 
  arrange(desc(varImp(fit_rf)$"Overall"))
```

TODO: explain the graph by mean decrease gini
```{r figure 17 plot variable importhance - rf, echo=FALSE}
varImp(fit_rf)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill = "lightblue")+
  coord_flip()+
  labs(title = "Variable importance for random forest model", 
       x = "characteristics", y = "variable importance")
```


## Model 3 - k-nearest neighbors 

### creating a new data frame 

creating a new data frame with just the first column of mushroom class (poison vs. edible)
```{r dataset for knn, include=FALSE}
mushrooms_break_down <- mushrooms[1:1]

# removing it from the original data frame as we don't want to iterate over it by mistake
mushrooms$class <- NULL

# iterating the column names in the original data frame (without the class column)
for (i in colnames(mushrooms)) {
  print(paste('going over column:', i))
  # Extracting factor levels from mushroom. Using [[]] in order to extract it as factor and not subset it as a dataframe.
  # more here:  https://rpubs.com/tomhopper/brackets and https://www.datamentor.io/r-programming/data-frame/
  # Note that we can't use '$' e.g. mushrooms$i as this format expects a real static name
  current_column_levels <- levels(mushrooms[[i]])
  print(paste('found levels:', list(current_column_levels)))
  for (j in levels(mushrooms[[i]])) {
    print(paste('adding column for level:', j))
    next_column_name = paste(i, '.', j, sep = "")
    next_column <- ifelse(mushrooms[i] == j,1,0)
    mushrooms_break_down[next_column_name] <- next_column
  }
}
```

```{r dataset for knn overview, echo=FALSE}
head(mushrooms_break_down[,1:7],10)
```
  * split the new data into training and test sets 
```{r split knn dataset, include=FALSE}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

test_index <- createDataPartition(y = mushrooms_break_down$class, times = 1, p = 0.2, list = FALSE)

train_set <- mushrooms_break_down %>% slice(-test_index)
test_set <- mushrooms_break_down %>% slice(test_index)

rm(test_index)

# define the outcome (class of the mushrooms) = y  
y <- train_set$class

# restore the true y from test set
y_test <- test_set$class

# remove the outcome from training and test set
test_set <- test_set %>% select(-class)
train_set <- train_set %>% select(-class)
```

### Train knn model and find best parameters 

```{r train knn }
set.seed(201, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(201)`
train_control <- trainControl(method="cv", number = 5, p = 0.8)
tune_grid <- data.frame(k = seq(1, 15, by=1))

set.seed(74, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(74)`
# Train knn model using "caret"
train_knn <- train(train_set, y, 
                   method = "knn", 
                   trControl = train_control,
                   tuneGrid = tune_grid)
# find optimal k
train_knn$bestTune
```

```{r figure 18 plot the selection of k by its accuracy, echo=FALSE}
plot(train_knn)
```
  * fit the model with best parameter using "caret"
```{r fit knn}
fit_knn <- knn3(train_set, y, 
                k = train_knn$bestTune$k)
```
  * predict the outcome
```{r predict knn}
y_hat_knn <- predict(fit_knn, test_set, type = "class")
```
  * Report accuracy 
```{r report knn accuracy }
cm_knn <- confusionMatrix(y_hat_knn, y_test)
cm_knn
```

```{r model 3 accuracy ,include=FALSE}
model_3_accuracy <- cm_knn$overall["Accuracy"]
```
### Model 3.1 - Increase the k to avoid overfitting 

  * fit Knn model with k=6
```{r fit Knn model with k6 }
set.seed(74, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(74)`
cm_knn_bigger_k <- knn3(train_set, y, k = 6)
```
  * predict the outcome
```{r predict knn bigger k }
cm_knn_bigger_k <- predict(cm_knn_bigger_k, test_set, type = "class")
```
  * Report accuracy 
```{r report knn begger k accuracy}
cm_knn_bigger_k <- confusionMatrix(cm_knn_bigger_k, y_test)
cm_knn_bigger_k
```

```{r model 3_1 accuracy ,include=FALSE}
model_3.1_accuracy <- cm_knn_bigger_k$overall["Accuracy"]
```

# Models results 

## summary of decision and rf 

```{r var imp comparison, echo=FALSE}
cbind.data.frame("rf"= varImp(fit_rf) %>%
                   mutate(char = rownames(.)) %>% 
                   arrange(desc(varImp(fit_rf)$"Overall")) %>%
                   slice(1:10), 
                 "ct"= varImp(fit_ct) %>%
                   mutate(char = rownames(.)) %>% 
                   arrange(desc(varImp(fit_ct)$"Overall")) %>%
                   slice(1:10),
                 "ct_tuned"= varImp(fit_ct_tuned) %>%
                   mutate(char = rownames(.)) %>% 
                   arrange(desc(varImp(fit_ct_tuned)$"Overall")) %>%
                   slice(1:10)) %>%
  knitr::kable()
```

## summary of the models accuracy

```{r models accuracy, echo=FALSE}
rbind("Classification Trees"= model_1_accuracy,
      "Classification Trees - tuned" = model_1.1_accuracy,
      "Random Forest" = model_2_accuracy,
      "Knn" = model_3_accuracy, 
      "Knn - bigger k" = model_3.1_accuracy) %>% 
knitr::kable()
```

# conclusion 

