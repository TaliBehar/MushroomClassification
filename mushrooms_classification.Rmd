---
title: "HarvardX; Data Science Capstone"
subtitle: 
  - "Choose Your Own Project"
  - "Mushroom Classification"
author: "Tali Behar"
date: "December 2020"
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center", out.width = '55%',
                      message=FALSE, warning=FALSE)
```
# Executive Summary

## Overview

During Covid-19 time, I started taking daily walks around the neighberhood, usually in the same path. I then started noticing an interesting sight, which is hunderds of mushrooms growing everytime after it rains (and I live in a very rainy area). They had different shapes, colors, texture and size, depending on where they grew. I read a bit in wikipedia about different kinds of mushrooms, in the hope that there is an easy way to determine whether a mushroom is edible (or even touchable), but it is far from being the case.
When I was searching for a topic for the CYO project, I came accross a huge dataset about mushrooms, and couldn't resist the opportunity to analyze it.

## Background
Our dataset is taken from: https://www.kaggle.com/uciml/mushroom-classification
This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. The latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like "leaflets three, let it be" for Poisonous Oak and Ivy.

The data is categorical- each column contain its own factors, represented by unique letters (see extension on section 2.3)

## Goal

1. Determine which features are most indicative of a poisonous mushroom.

2. Determine what types of ML models perform best on 'Mushroom Classification' dataset by achieving model accuracy of 100%, in a reasonable runtime. 

## Variable Selection

1.	**Y = class**

  The outcome will be the class of the mushroom: e = edible, p = poisonous 

2.	**The features**

  For classifying mushrooms, whether it's outside or through images, it's important to be familiar with all the details about them. I chose to take all of the 22 mushrooms characteristics to be used for the prediction: Odor, population,	habitat, bruises, cap, gill, stalk, veil, ring and spore-print-color.
    
## Method 
  
The ML algorithms that were chosen to model the mushroom classification dataset come from the family of **Classification Algorithms** and are: Random forest, Classification trees, and K-nearest neighbors (KNN).    

 * Divide the mushrooms dataset randomly to train set and test set. 
 
 * Visualize the distribution of the characteristics between e/p class and find meaningful insights.
 
 * Visualize the relationships and correlation in the data - once between the characteristics and the class, and once among the characteristics themselves. 
 
 * Train and fit the above algorithms using cross-validation and tuning methods.
 
 * Omit the unimportant features.
 
 * Construct canonical dataset for applying the KNN algorithm.  

## Model Estimation

Evaluate model performance based on its **overall accuracy** and runtime.

Overall accuracy is the simplest way to evaluate the algorithm when the outcomes are categorical. This method is simply reporting the proportion of cases that were correctly predicted in the test set.  

## Model Results 

Random forest, KNN and Classification trees (model 1.2) achieved the perfect 100% accuracy.

## Conclusion

* The chosen models for predicting the mushrooms class are Random Forest and KNN, which are more flexible and easier to train and tune, and are better for classifying. 

* Random forest performed better runtime- 422% faster than KNN runtime.

* The most important features for the classification are 'spore.print.color', 'gill.color', 'stalk.surface.above.ring' and 'stalk.surface.below.ring' 
 
 
# Data Preparation

## Installing Required Packages

```{r required packages, eval=FALSE, include=FALSE}
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(plot.matrix)) install.packages("plot.matrix", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
```

```{r install required packages, echo=TRUE}
library(readr)
library(tidyverse)
library(caret)
library(gridExtra)
library(knitr)
library(plot.matrix)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(randomForest)
```

## Loading Mushrooms Classification Dataset

  * Note: this project was developed on R version 3.6.1 and might not be compatible with newer versions
```{r}
# Importing the data - read csv file locally
# Note: original dataset can be found here: https://www.kaggle.com/uciml/mushroom-classification
# Getting the path of your current open file
mushrooms_df <- read.csv("mushrooms.csv")
mushrooms <- mushrooms_df
```
  
## Get A Glimpse Of Mushrooms Classification Dataset
```{r dim table, echo=FALSE}
tibble("class"=class(mushrooms), "nrow"=nrow(mushrooms), "ncol"=ncol(mushrooms)) %>%
  knitr::kable()
```

```{r glimpse of the data, echo=FALSE}
glimpse(mushrooms)
```
### The mushrooms datadet contains 8k rows and 23 columns. 

**columns** 

The first column represents the class of the mushroom, "e" for edible and "P" for poisonous.

Columns 2:23 represents the mushrooms characteristics as the following: 

Odor, Population,	Habitat, Bruises, Cap, Gill, Stalk, Veil, Ring and Spore-print-color. 

We can distinguish between visual and general characteristics: 

  * Visual characteristics can be measured by sight (images), such as the color and shapes of the mushroom's cap, bruises, gill, stalk, veil and ring. 

  * General characteristics must be evaluated in-person, such as the mushroom's odor, the habitat, population and Spore-print-color. 

**Rows** 

Each row represents one observation on one mushroom and contain its class and unique characteristics.


Moving forward, we'll learn about the mushroom’s characteristics and their influence on the mushroom’s classification.

### The dataset is categorical
  
The next table specifies the class of the outcome and features (factor), and the levels of each of the factors. 
  
'veil.type' has only 1 level which is not contributing information as a feature. We'll remove it from the data and won't use it as a feature. 

Some observations contain '?' in 'stalk.root' column, and we chose to use it as a regular factor, as the distribution is equal between edible and poisonous, and if we omitted these rows, we would lose 40% of the data.
```{r class and levels summarize , echo=FALSE}
cbind.data.frame("mushrooms characteristics"= names(mushrooms),
                 "class"= sapply(mushrooms,(function(x){class(x)})), 
                 "levels"= sapply(mushrooms,function(x){nlevels(x)})) %>% 
  mutate(char = rownames(.)) %>% 
  select(-char)%>%
  knitr::kable()
```

```{r remove veil type col, include=FALSE}
mushrooms$veil.type<-NULL
```
### The prevalence of the observation

The prevalence of the mushrooms class in the dataset is almost about the same.
  
52% of the observations belong to edible type and 48% are poisonous, which means we should not expect biased prediction. 

Predicting poisonous mushrooms as edible can be a terrible mistake, so in addition to the accuracy of the model, we'll also look at the other parts of the 'confusionMatrix', that contains the sensitivity, specificity, prevalence and balanced accuracy.    
```{r mushroom type, echo=FALSE}
mushrooms %>% 
  group_by(class) %>% 
  summarize (count=n())%>%
  mutate(prop=100*count/sum(count)) %>%
  setNames(c("Mushroom class","Count", "Proportion (%)")) %>%
  knitr::kable()
```

Here is a quick summary of the variables and their count. 

Since the full table is very wide, we'll present only the first 5 columns
```{r dataset summary, echo=FALSE}
mushrooms[,1:5] %>%
  summary()
```

## Create training and test sets
```{r create train and test}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# Test set will be 20% of mushrooms data
test_index <- createDataPartition(y = mushrooms$class, 
                                  times = 1, p = 0.2, list = FALSE)

train_set <- mushrooms %>% slice(-test_index)
test_set <- mushrooms %>% slice(test_index)
# Remove test_index
rm(test_index)

# Define the outcome (class of the mushrooms) = y  
y <- train_set$class

# Restore the true y from test set
y_test <- test_set$class

# Remove the outcome from training and test set
test_set <- test_set %>% select(-class)
train_set <- train_set %>% select(-class)
```

# Mushrooms classification Dataset Overview 

## Visual Characteristics

### cap and bruises characteristics

 * Cap
  
    - Differently shaped and colored upper part of the mushroom that protects the gills. In other words, the cap is the top of the mushroom. 

    - Cap shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s

    - Cap surface: fibrous=f, grooves=g, scaly=y, smooth=s

    - Cap color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y


  * Bruises

    - Mushroom bruising involves nicking the top and bottom of the mushroom cap and observing any color changes.

    - Bruises: bruises=t, no=f
    
    
  * We can see that there is no significant difference in the cap characteristics or bruises that indicate class. Yet, sunken shape, purple color and green color will always indicate edible mushroom.

```{r figure 1 cap characteristics, echo=FALSE, out.width = '80%'}
cap_char <- 
  lapply(names(train_set[,1:4]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 4 cap figures in 2 columns and 2 rows 
grid.arrange(cap_char[[1]],cap_char[[2]],cap_char[[3]],cap_char[[4]],
             ncol=2, nrow=2)
```
 
### gill characteristics

  * Fertile spore-producing part of the mushroom, located under the cap. 

    - Gill attachment: attached=a, descending=d, free=f, notched=n

    - Gill spacing: close=c, crowded=w, distant=d

    - Gill size: broad=b, narrow=n

    - Gill color: black=k, brown=n, buff=b, chocolate=h, gray=g,
    green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y
 
 
  * We can see differences in the gill characteristics that may indicate the mushroom's class. Buff and Green gill colors will always indicate poisonous.   
```{r figure 2 gill characteristics, echo=FALSE, out.width = '80%'}
gill_char <- 
  lapply(names(train_set[,6:9]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 4 gill figures in 2 columns and 2 rows 
grid.arrange(gill_char[[1]],gill_char[[2]],gill_char[[3]],gill_char[[4]],
             ncol=2, nrow=2)
```

### Stalk characteristics

  * Stem, Axis supporting the mushroom’s cap. The role of the mushroom stalk is to raise the cap above the grass, twigs or stones that are close to the ground.
  
    - Stalk shape: enlarging=e, tapering=t

    - Stalk root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?

    - Stalk surface above ring: fibrous=f, scaly=y, silky=k, smooth=s

    - Stalk surface below ring: fibrous=f, scaly=y, silky=k, smooth=s

    - Stalk color above ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y

    - Stalk color below ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y


  * Buff, cinnamon, and yellow stalk color above ring will always indicate poisonous mushroom. All other stalk characteristics distributing in different frequency between the mushrooms class.  

```{r figure 3 stalk characteristics, echo=FALSE, out.width = '80%'}
stalk_char <- 
  lapply(names(train_set[,10:15]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
})
# plot all 6 stalk figures in 2 columns and 3 rows 
grid.arrange(stalk_char[[1]],stalk_char[[2]],stalk_char[[3]],
             stalk_char[[4]],stalk_char[[5]],stalk_char[[6]],
             ncol=2, nrow=3)
```

### veil and Ring characteristics

  * Veil
  
    - The thin membrane that covers the cap and stalk of an immature mushroom

    - Veil color: brown=n, orange=o, white=w, yellow=y
    
    
  * Ring
  
    - Membrane located under the cap and circling the stem; remnant of a membrane that covered the gills of the immature mushroom and ruptured as the cap grew.

    - Ring number: none=n, one=o, two=t

    - Ring type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z


  * Brown and orange veil color will always indicate edible mushroom.
  
  * Mushrooms with large ring or no rings will always indicate poison.   
```{r figure 4 veil ring characteristics, echo=FALSE, out.width = '80%'}
veil_ring_char <- 
  lapply(names(train_set[,16:18]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
  })
# plot all 3 veil figures in 2 columns and 2 rows 
grid.arrange(veil_ring_char[[1]],veil_ring_char[[2]],
             veil_ring_char[[3]], ncol=2, nrow=2)
```

## General Characteristics

 * Odor - The smell of the mushroom

    - almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s
    
    - There are significant differences in the odor distribution. Poisonous and edible mushrooms smelles very differently and we can consider the odor as a dominant feature. 
    
    
  * Spore print color 
  
    - The spore print is the powdery deposit obtained by allowing spores of a fungal fruit body to fall onto a surface underneath
  
    - black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y
    
    - Green print color will always indicate poisonous.  
    
    
  * Population - the way they spread in their habitat 
  
    - abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y
    
    - Poisonous mushrooms can never be found abundant or in numerous numbers.
  
  
  * Habitat
  
    - grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d
    
    - Poisonous mushrooms can grow anywhere but can never be found on waste.

```{r figure 5 other characteristics, echo=FALSE, out.width = '80%'}
other <- 
  lapply(names(train_set[,c(5,19:21)]),function(x){
    train_set %>%
      ggplot(aes_string(y,x,fill=y,color=y))+
      geom_jitter()+
      theme(legend.position = "none")
  })
# plot 4 assorted figures in 2 columns and 2 rows 
grid.arrange(other[[1]],other[[2]],other[[3]],other[[4]], ncol=2, nrow=2)
```
 
## Relationships and correlation in the data 

All the above characteristics are combining one mushroom. Since that, it can be assumed that all the features are correlated with the class of the mushroom and among themselves.

In the above plots we noticed high influence of the odor on the mushroom class. 
Yet, in order to select the best other features that have a strong predictive power, we need to see if they are correlated or not.  

For building efficient predictive models, we would ideally only include variables that uniquely explain some amount of variance in the outcome.

### Chi-Square Test

The **Chi-Square Test** (${\chi}^2$ test) of Independence determines whether there is an association between **categorical variables**.

It compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each other. 

It is a nonparametric test.

A chi square test will produce a p-value which will tell us if the test results are significant or not.

Under the null hypothesis $H_{0}$ - the categorical variables are independent.

If $p.value <= {\alpha}$: significant result, reject $H_{0}$, dependent.

If $p.value > {\alpha}$: not significant result, fail to reject $H_{0}$, independent.

We'll set $p.value = 0.05$ 

### Relationships between the characteristics by ${\chi}^2$ test 
```{r car p.value matrix, include=FALSE}
index <- expand.grid(1:21,1:21) 
chi2_test_for_char <- 
  sapply(1:nrow(index), function(i){
  n <- index[i,1]
  m <- index[i,2]
  chisq.test(train_set[,n],train_set[,m],
             simulate.p.value = TRUE, B = 1000)$p.value
}) %>% 
  matrix(nrow=21, byrow = FALSE)
```
The rows and the columns represent the 21 features. The diagonal represents the correlation of the feature with itself (always correlated).  

We can see that all the features correlated with each other except 'veil color' and 'ring number' that are independent from each other  
```{r figure 6 car p.value matrix, echo=FALSE, out.width = '80%'}
plot(chi2_test_for_char, 
     col=c("white", "lightblue"),
     breaks=c(0, 0.05, 1),
     key=list(side=3, cex.axis=0.6))
```

### Relationships between the characteristics to class by ${\chi}^2$ test 
```{r car calss p.value matrix, include=FALSE} 
chi2_test_for_class <- 
  sapply(1:21, function(i){
  chisq.test(train_set[,i],y,
  simulate.p.value = TRUE, B = 1000)$p.value # B= number of monte-carlo simulations 
}) %>% 
  matrix(ncol=21, byrow = FALSE)
```
All the characteristics are correlated with the mushroom class 
```{r figure 7 car class p.value matrix, echo=FALSE, out.width = '60%'}
plot(chi2_test_for_class, 
     col=c("white", "lightblue"), 
     breaks=c(0, 0.05, 1),
     key=list(side=3, cex.axis=0.6))
```

# Methods and Analysis

A classification algorithm is a function that weighs the input features so that the output separates one class into positive values and the other into negative values. Classifier training is performed to identify the weights that provide the most accurate and best separation of the two classes of data.

Among the common classification algorithms are classification/decision trees (ct), Random forest (rf), and K-nearest neighbor (KNN). The latter works slightly different than the first two. 

Since the correlation matrix showed that the features are dependent, we will start with all the 21 features. 

## Model 1 - Classification trees

  * Classification / decision trees, are used in prediction problems where the outcome is
categorical. We form predictions by calculating which class is the most common
among the training set observations within the partition.

  * The default **splitting** method for classification called "Gini Impurity" ("Gini Index") and is the following: 
  
$$ Gini(j) = {\displaystyle\sum_{k=1}^K\hat{p}_{j,k}(1-\hat{p}_{j,k})} $$
We define $\hat{p}_{j,k}$= the proportion of observation in partition ${j}$ that are in class ${k}$

"Gini Impurity" seek to partition observation into subset that have the same class, when all observations in a group fall into single category, then $Gini(j)=0$  

```{r eval=FALSE, include=FALSE}
# (https://www.edureka.co/blog/implementation-of-decision-tree/) 
# http://www.milbo.org/rpart-plot/prp.pdf # rpart plots examples
# https://csantill.github.io/RTuningModelParameters/
```
  * Train ct model and find best parameters
  
In order to avoid overfitting, we'll use 10-folds cross validation. The splits are done by 80%-20% training and test set. 
```{r train ct model}
train_control <- trainControl(method="cv", number = 10, p = 0.8) # use 10-folds cross-validation 
tune_grid <- data.frame(cp = seq(0.0, 0.2, len=30)) # find the best value for cp

# Train using "caret"
train_ct <- 
  train(train_set, y, 
        method = "rpart", 
        tuneLength = 6,
        trControl = train_control,
        tuneGrid = tune_grid)
# Find best cp value
train_ct$bestTune
```
The complexity parameter (cp) is the minimum improvement in the model that is needed in each node (the Gini impurity must improve by a factor of cp for the new partition to be added). 

When cp=0 then the tree is fully grown and no need to prune it. Large values of cp will force the algorithm to stop earlier which results in fewer nodes. 
```{r figure 8 Plot the cp selection by its accuracy, echo=FALSE}
plot(train_ct)
```
  * Fit the model with the best complexity parameter 
```{r fit ct model}  
fit_ct <- 
  rpart(y ~ ., 
        data = train_set, 
        method = "class", 
        control = rpart.control(cp = train_ct$bestTune$cp , minsplit = 20))
```

```{r figure 9 Plot size of the tree (nodes) by cp, echo=FALSE}
plotcp(fit_ct)
```
The above graph shows that the cross validation relative error (X-val) reaches the minimal value for a tree of size 6, which corresponds to a complexity parameter of 0. 

The final tree is the following. The top node contain the entire sample, each of the remaining nodes contain a subset of the sample in the node directly above it. 

As we predicted before, 'Odor' is a dominant feature and is the first to be splitted, the next feature to be splitted is 'Spore print color', while the last is 'Stalk root'. 

Notice that the algorithm splitted 4 features out of 21. 
```{r figure 10 Plot the tree, echo=FALSE, out.width = '75%'}
rpart.plot(x = fit_ct, type =5, extra = 100, 
           box.palette = c("lightblue","orangered"), 
           fallen.leaves=TRUE, tweak = 1)
```
    * Variable importance 

The importance of the variable calculated by "Gini Importance" or "mean decrease gini" 

"Gini importance" calculates each feature importance as the sum over the number of splits that include the feature, proportionally to the number of samples it splits.

We can interpret it as how valuable or needed this variable to the split.
We can see that 8 out of the 21 features did not contribute any information to the splits , but "Stalk" characteristics had valuable influence overall.  
```{r figure 11 Graph variable importance ct, echo=FALSE, out.width = '70%'}
varImp(fit_ct)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill="lightblue")+
  coord_flip()+
  labs(title = "Variable importance for classification trees model", 
       x = "characteristics", y = "variable importance")
```
  * Predict the outcome
```{r predict ct }
y_hat_ct <- predict(fit_ct, test_set, type = "class")
```
  * Report accuracy 
```{r report ct accuracy, echo=FALSE}
cm_ct <- confusionMatrix(y_hat_ct, y_test)
cm_ct$table 
model_1_accuracy <- cm_ct$overall["Accuracy"]
rbind(c(model_1_accuracy, 
         cm_ct$byClass[c("Sensitivity","Specificity","Prevalence","Balanced Accuracy")])) %>%
  knitr::kable()

```
  * Model result

Classification trees model achieved "only" 0.998 of accuracy, with false-positive mistake of 3 edible observation that predicted as poisonous In true-life terms, this mistake is forgiven, but it wasn't if the opposite has occurred. 

## Model 1.1 - Classification trees - only "Visual Characteristics" 

The previous model took into consideration all the variables and we saw that 2 of the general characteristics played a significant role in the splitting process and variable importance. Yet, we got accuracy of 99.8% and false-positive mistake of 3 observations. 

Let's investigate a case which we are asking to classify a mushroom by an image. What would be the best features for the classification? 

  * Create new dataset, omit the general characteristics
```{r new_mushrooms dataset}
visual_mushrooms <- 
  mushrooms %>% 
  select(-odor, -habitat, -population, -spore.print.color)
```
  * We'll split the new data into training and test sets 
```{r split the new data, include=FALSE}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# split the new data into training and test sets 
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# Test_visual will be 20% of visual_mushrooms data
visual_test_index <- createDataPartition(y = visual_mushrooms$class, 
                                         times = 1, p = 0.2, list = FALSE)

train_visual <- visual_mushrooms %>% slice(-visual_test_index)
test_visual <- visual_mushrooms %>% slice(visual_test_index)

# Remove test_index
rm(visual_test_index)

# Define the outcome (class of the mushrooms) = y_new  
y_visual <- train_visual$class

# Restore the true y from test_new
y_test_visual <- test_visual$class

# Remove the outcome from training and test_new
test_visual <- test_visual %>% select(-class)
train_visual <- train_visual %>% select(-class)
```

  * Train tuned ct model and find best parameters

We'll repeat the previous steps and train the model to find the best complexity parameter with 10-folds cross-validation to avoid overfitting. 
```{r Train tuned ct using caret }
train_control <- trainControl(method="cv", number = 10, p = 0.8)
tune_grid <- data.frame(cp = seq(0.0, 0.2, len=30))
train_ct_visual <- train(train_visual, y_visual, 
                  method = "rpart", 
                  tuneLength = 6,
                  trControl = train_control,
                  tuneGrid = tune_grid)
# Find best cp value
train_ct_visual$bestTune
```
  * Fit the model with best parameter 
```{r Fit tuned ct using rpart}
fit_ct_visual <- 
  rpart(y_visual ~ ., 
        data = train_visual, 
        method = "class", 
        control = rpart.control(cp = train_ct_visual$bestTune$cp , minsplit = 20))
```
The cross validation relative error (X-val) reaching the minimal value for a tree of size 20, which corresponds to a complexity parameter of 0. 
This tree pruned with 16 more branches than the original tree. 
```{r figure 12 Plot size of the tree (nodes) by cp, echo=FALSE}
plotcp(fit_ct_visual)
```
  * The 'visual' tree 
```{r figure 13 plot tuned tree, echo=FALSE, out.width='75%'}
rpart.plot(x = fit_ct_visual, type =5, extra = 100, 
           box.palette = c("lightblue","orangered"), 
           fallen.leaves=TRUE, tweak = 1)
```
 * Variable importance  

Stalk still have large influence on the splitting, while ring type has the highest 'mean decrease Gini' (used to be zero in the original tree), means that gill type (the first var. to split) and the ring type is highly correlated.
```{r figure 14 variable importance tuned ct tuned, echo=FALSE, out.width='70%'}
varImp(fit_ct_visual)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill="lightblue")+
  coord_flip()+
  labs(title = "Variable importance for classification trees - visual model", 
       x = "characteristics", y = "variable importance")
```
  * Predict the outcome
```{r predict tuned ct}
y_hat_ct_visual <- predict(fit_ct_visual, test_visual, type = "class")
```
  * Report accuracy 
```{r report tuned ct accuracy, echo=FALSE}
cm_ct_visual <- confusionMatrix(y_hat_ct_visual, y_test_visual)
cm_ct_visual$table 
model_1.1_accuracy <- cm_ct_visual$overall["Accuracy"]
rbind(c(model_1.1_accuracy, 
         cm_ct_visual$byClass[c("Sensitivity","Specificity","Prevalence","Balanced Accuracy")])) %>%
  knitr::kable()
```
  * Model results
  
The visual model achieved lower result than the original - 99.6% accuracy.  

There is 5 false-positive results (5 edible predicted poisonous) and one false-negative result (poisonous predicted as edible). 

Means, one of the general characteristics is essential for the classification process. 

```{r remove the next objects, include=FALSE}
rm(visual_mushrooms, test_visual, train_visual, y_visual, y_test_visual)
```

# Model 1.2 - Classification trees - tuned

  * Create new dataset, omit the general characters, return spore.print.color
```{r tuned_mushrooms}
tuned_mushrooms <- 
  mushrooms %>% 
  select(-odor, -habitat, -population)
```

```{r split tuned, include=FALSE}
#split the new data into training and test sets 
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

# Test_tuned will be 20% of tuned_mushrooms data
tuned_test_index <- createDataPartition(y = tuned_mushrooms$class, times = 1, p = 0.2, list = FALSE)

train_tuned <- tuned_mushrooms %>% slice(-tuned_test_index)
test_tuned <- tuned_mushrooms %>% slice(tuned_test_index)
# Remove test_index
rm(tuned_test_index)

# Define the outcome (class of the mushrooms) = y_new  
y_tuned <- train_tuned$class

# Restore the true y from test_new
y_test_tuned <- test_tuned$class

# Remove the outcome from training and test_new
test_tuned <- test_tuned %>% select(-class)
train_tuned <- train_tuned %>% select(-class)
```
  * Train tuned ct model and find best parameters
```{r train and fit tuned, include=FALSE}
# Train using "caret"
train_control <- trainControl(method="cv", number = 10, p = 0.8)
tune_grid <- data.frame(cp = seq(0.0, 0.2, len=30))
train_ct_tuned <- 
  train(train_tuned, y_tuned, 
        method = "rpart", 
        tuneLength = 6,
        trControl = train_control,
        tuneGrid = tune_grid)
# Find best cp value
train_ct_tuned$bestTune

# Fit the model with best parameter 
# Fit using "rpart"
fit_ct_tuned <- 
  rpart(y_tuned ~ ., 
  data = train_tuned, 
  method = "class", 
  control = rpart.control(cp = train_ct_tuned$bestTune$cp , minsplit = 20))
```
  * Plot the tree 
```{r figure 15 tuned tree plot, echo=FALSE, out.width='75%'}
rpart.plot(x = fit_ct_tuned, type =5, extra = 100, 
           box.palette = c("lightblue","orangered"), 
           fallen.leaves=TRUE, tweak = 1)
```
  * Graph variable importance  
```{r figure 16 tuned variable importance , echo=FALSE, out.width='70%'}
varImp(fit_ct_tuned)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill="lightblue")+
  coord_flip()+
  labs(title = "Variable importance for classification trees - tuned model", 
       x = "characteristics", y = "variable importance")
```

```{r ct tuned var imp, include=FALSE}
# var importance list
ct_tuned_var_imp <- 
  varImp(fit_ct_tuned) %>%
  mutate(char = rownames(.)) %>% 
  arrange(desc(varImp(fit_ct_tuned)$"Overall")) %>%
  slice(1:15) %>% 
  .$char
```

  * Predict and report accuracy
```{r Predict Report accuracy tuned, echo=FALSE}
# Predict the outcome
y_hat_ct_tuned <- predict(fit_ct_tuned, test_tuned, type = "class")

# Report accuracy 
cm_ct_tuned <- confusionMatrix(y_hat_ct_tuned, y_test_tuned)
cm_ct_tuned$table 
model_1.2_accuracy <- cm_ct_tuned$overall["Accuracy"]
rbind(c(model_1.2_accuracy, 
        cm_ct_tuned$byClass[c("Sensitivity","Specificity","Prevalence","Balanced Accuracy")])) %>%
  knitr::kable()
```
  * Model results
  
The model achieved a perfect accuracy of 100%. 
It can be assumed that 'Odor' that was a dominant feature in the original model was "stand in the way" towards a perfect prediction. Smell can be very confusing sometimes. 

In addition, the model suggests that 'gill attachment', 'veil color', and 'veil type' do not contribute information to the model and therefore are not needed, alongside with 'odor', 'population' and 'habitat' 

```{r remove objects, include=FALSE}
# remove the next objects
rm(tuned_mushrooms,test_tuned, train_tuned, y_tuned, y_test_tuned)
```

  * Advantages of Classification trees
  
Classification trees are highly interpretable and easy to visualize (if small enough). They can model human decision processes and don’t require use of dummy predictors for categorical variables.

  * Classification trees limitations

The approach via recursive partitioning can easily over-train and is therefore a bit harder to train. In terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. That is why we saw such big differences in the above models (particularly in the size of the trees), when we omitted/added features. 


## Model 2 - Random Forest

The Random forests combines multiple Classification trees. Its goal is to improve prediction performance
and reduce instability by averaging multiple Classification trees.

  * Train rf model and find best parameters

Same here, we will use 10-folds cross validation and set a tune_grid for "mtry" (number of variables randomly sampled as candidates at each split). We can see in the next plot that the accuracy is higher when "mtry" reaches to 4 randomly sampled variables.  
```{r train rf model}
set.seed(3, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(3)`

train_control <- trainControl(method="cv", number = 10, p = 0.8) # use 10-folds cross-validation 
tune_grid <- data.frame(mtry =  c(1:5)) # find the best value for tuning mtry

set.seed(13, sample.kind="Rounding") # if using R 3.5 or earlier, use 'set.seed(13)'

train_rf <- 
  train(train_set, y,
        method = "rf",
        ntree = 200,  #number of trees to grow
        trControl = train_control,
        tuneGrid = tune_grid)
# Find best mtry parameter
train_rf$bestTune
```

```{r figure 17 Plot the mtry selection by its accuracy, echo=FALSE}
plot(train_rf)
```
  * Fit the model with best parameter 
```{r fit rf model}
fit_rf <- 
  randomForest(train_set, y,
               ntree = 200, 
               nodesize = train_rf$bestTune$mtry)
``` 
The accuracy improves as we added more trees until about 20 trees where accuracy stabilized
```{r figure 18 Plot ntree, echo=FALSE}
plot(fit_rf)
```
Random forest calculates the variable importance over ensembles of randomized trees.

The Random Forest first important variable is 'Odor'. In addition, all general characteristics are participating in the classification and have a predictive power. 

Down the list, the less important features 'gill attachment' and 'veil color'. Those 2 features also weren't needed in the tuned ct model that got 100% accuracy. Meaning, we can omit them. 
```{r figure 19 plot variable importance rf, echo=FALSE, out.width='70%'}
varImp(fit_rf)%>%
  mutate(char = rownames(.))%>%
  ggplot(aes(x = reorder(char,Overall),y = Overall))+
  geom_bar(stat = "identity", fill = "lightblue")+
  coord_flip()+
  labs(title = "Variable importance for random forest model", 
       x = "characteristics", y = "variable importance")
```
  
```{r rf var importance list, include=FALSE}
# var importance list 
rf_var_imp <- 
  varImp(fit_rf) %>%
  mutate(char = rownames(.)) %>% 
  arrange(desc(varImp(fit_rf)$"Overall")) %>%
  slice(1:15) %>% 
  .$char
``` 
  * Predict the outcome 
```{r predict rf}  
y_hat_rf <- predict(fit_rf, test_set, type = "class")
```
  * Report accuracy 
```{r report rf accuracy, echo=FALSE}
cm_rf <- confusionMatrix(y_hat_rf, y_test)
cm_rf$table 
model_2_accuracy <- cm_rf$overall["Accuracy"]
rbind(c(model_2_accuracy, 
       cm_rf$byClass[c("Sensitivity","Specificity","Prevalence","Balanced Accuracy")])) %>%
  knitr::kable()
```
  * Model results

Random forest achieved 100% accuracy.

Random forest chooses features randomly during the training process. Therefore, it does not depend highly on any specific set of features. It is more difficult to interpret than a single classification tree, but the randomized feature selection makes random forest much more accurate than a decision tree.

Therefore, rf could handle 'Odor' better than the simple first model of classification trees. 

In addition, both rf and ct did not need 'gill attachment' and 'veil color' as a predictors for a perfect accuracy.

  * Random forest limitations 

The more accurate prediction requires more trees while large number of trees can make the algorithm too slow and ineffective for real-time predictions.

With 10-folds cross validation, the overall runtime of the model was 24 seconds: 21 seconds for training, less than one second for fitting, and less than one second for predicting. 

When dealing with big amount of data to train and predict, it's faster to use the 'Rborist' package rather than 'randomForest'

## Model 3 - k-nearest neighbors 

K-nearest neighbors is a pattern recognition algorithm that uses training datasets to find the k closest relatives in future examples. 

When KNN is used in classification with categorical data, the basic idea is that each category is mapped into a real number (dummy) in some optimal way, and then KNN classification is performed using those numeric values. 

  * For this model will use the conclusions from the previous models that achieved 100% accuracy and omit the two unnecessary features.

  * Creating a new data set 
  
Since KNN cannot handle characters, we'll convert the mushrooms data set into canonical data set, as the following overview. Then, we'll split the data into training and test sets. 
```{r dataset for KNN, include=FALSE}
# use the original dataset to create sanitized dataset omitted the 2 features and the veil type we dropped at the beginning
mushrooms_sanitized <-  mushrooms_df %>% select (-gill.attachment, -veil.color, -veil.type)

# creating a new data frame with just the first column of mushroom class (poison vs. edible)
mushrooms_break_down <- mushrooms_sanitized[1:1]

# removing it from the original data frame as we don't want to iterate over it by mistake
mushrooms_sanitized$class <- NULL

# iterating the column names in the original data frame (without the class column)
for (i in colnames(mushrooms_sanitized)) {
  print(paste('going over column:', i))
  # Extracting factor levels from mushroom. Using [[]] in order to extract it as factor and not subset it as a dataframe.
  # more here:  https://rpubs.com/tomhopper/brackets and https://www.datamentor.io/r-programming/data-frame/
  # Note that we can't use '$' e.g. mushrooms$i as this format expects a real static name
  current_column_levels <- levels(mushrooms_sanitized[[i]])
  print(paste('found levels:', list(current_column_levels)))
  for (j in levels(mushrooms_sanitized[[i]])) {
    print(paste('adding column for level:', j))
    next_column_name = paste(i, '.', j, sep = "")
    next_column <- ifelse(mushrooms_sanitized[i] == j,1,0)
    mushrooms_break_down[next_column_name] <- next_column
  }
}

```

```{r dataset for KNN overview, echo=FALSE}
# overview of the first 7 rows of the new datadet
head(mushrooms_break_down[,1:7],10)
```
  * We'll split the new data set into training and test sets 
```{r split KNN dataset, include=FALSE}
# split the new data into training and test sets 
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

test_index <- createDataPartition(y = mushrooms_break_down$class, times = 1, p = 0.2, list = FALSE)

train_set <- mushrooms_break_down %>% slice(-test_index)
test_set <- mushrooms_break_down %>% slice(test_index)

rm(test_index)

# define the outcome (class of the mushrooms) = y  
y <- train_set$class

# restore the true y from test set
y_test <- test_set$class

# remove the outcome from training and test set
test_set <- test_set %>% select(-class)
train_set <- train_set %>% select(-class)
```

  * Train KNN model and find best parameters

We'll use here 5-folds cross-validation to avoid overfitting, since setting the number of folds to 10 increased the model runtime in 140% and achieved the same model accuracy.

In addition, we'll set a tune_grid for K in order to find the optimal parameter
```{r train KNN }
set.seed(201, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(201)`
train_control <- trainControl(method="cv", number = 5, p = 0.8)
tune_grid <- data.frame(k = seq(1, 15, by=1))

set.seed(74, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(74)`
# Train KNN model using "caret"
train_knn <- 
  train(train_set, y, 
        method = "knn", 
        trControl = train_control,
        tuneGrid = tune_grid)
# find optimal k
train_knn$bestTune
```
The model achieved the highest accuracy with k=3, meaning 3 variables in each neighborhood.

In case of low accuracy, we can suspect the lowest K as overtraining of the model
```{r figure 20 plot the selection of k by its accuracy, echo=FALSE}
plot(train_knn)
```
  * Fit the model with best parameter
```{r fit KNN}
fit_knn <- knn3(train_set, y, 
                k = train_knn$bestTune$k)
```
  *Predict the outcome and report accuracy
```{r predict KNN, echo=FALSE}
y_hat_knn <- predict(fit_knn, test_set, type = "class")

cm_knn <- confusionMatrix(y_hat_knn, y_test)
cm_knn$table
model_3_accuracy <- cm_knn$overall["Accuracy"]
rbind(c(model_3_accuracy, 
        cm_knn$byClass[c("Sensitivity","Specificity","Prevalence","Balanced Accuracy")])) %>%
  knitr::kable()
```
  * Model results

The low K=3 could lead us to an overtraining of the model, but even though and with the omission of two unnecessary features, KNN achieved 100% accuracy.  

  * limitation

  1. With 5-folds cross validation, the overall runtime of the model was 97 seconds: 94 seconds for training, less than one second for fitting, and two seconds for predicting (10-folds cv increased the total runtime in 40 seconds and achieved the same model accuracy)

  2. Since KNN is based on the Euclidean distances, it unable to be applied directly on categorical data. In order to define the distance metrics for categorical variables, we need to preprocess the dataset by transforming it to dummy variables that represent the categorical variables.



# The Models Results 

## The models accuracy

Random forest, KNN and Classification trees (1.2 tuned model - variables omitted) achieved the perfect 100% accuracy.

The first two Classification trees models (1, 1.1) achieved high, but not enough, accuracy while the latter got a fatal false-negative mistake by classifying poisonous as edible. 

```{r models accuracy, echo=FALSE}
rbind("Classification Trees"= model_1_accuracy,
      "Classification Trees - only 'Visual Characteristics'" = model_1.1_accuracy,
      "Classification Trees - tuned" = model_1.2_accuracy,
      "Random Forest" = model_2_accuracy,
      "KNN" = model_3_accuracy) %>% 
knitr::kable()
```
## The 15 most important features according to the 1.2 Classification trees model and Random forest

The next table represent the top 15 important variables (out of initial 21).

Each one of the following models consider different features as most important. 

Yet, 'spore.print.color' and 'gill.color' have large contribution to both of the models (those are highly correlated, since the gill is the part that produces the spore and it is only reasonable to share the color). 

In addition, 'gill.attachment' and 'veil.color' omitted from KNN and Classification trees model while playing a very negligible, if any, role in random forest classifier.
```{r var imp comparison, echo=FALSE}
print(data.frame("Classification trees"=ct_tuned_var_imp,"Random forest"=rf_var_imp))
```

# Conclusion

  * What types of machine learning models perform best on this dataset?

Analyzing and modeling the Mushrooms classification dataset lead us to a conclusion that models such as KNN and Random forest, which are more flexible and easier to train and tune, are better for classifying.

The limitation of each of the KNN and rf models that discussed at the end of each section, did not hurt or biased the results. In addition, rf performed better runtime- 422% faster than KNN runtime. 

For classification trees, in my opinion, the model couldn't handle well with the task, since we needed some rough adjustments, including omitting the most important var ('Odor') in order to achieve 100% accuracy. Second, the model was very sensitive to changes and changed the size of the trees in a significant way.              

  * Which features are most indicative of a poisonous mushroom?

We can see that omitting 'gill.attachment' and 'veil.color' from **all** of the models led us to 100% accuracy. 

The common 4 top important features from Classification trees (1.2) model and rf model are 'spore.print.color', 'gill.color', 'stalk.surface.above.ring', 'stalk.surface.below.ring' and we must not omit them. 

  * Future work

As for the 'Odor', there is no certainty about its roll. 

I'd like to test some more classification algorithms such as 'Logistic Regression' and 'Support Vector Machines' to better determine which algorithms perform best under their limitations and create a better stable list of features importance.  


